
import string
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps=PorterStemmer()
wnl=WordNetLemmatizer()
example_sent="It is proposed by Lovins in 1968, that removes the longest suffix from a word then the word is recorded to convert this stem into valid words."
example_sent_no_punct=example_sent.translate(str.maketrans("","",string.punctuation))
word_tokens=word_tokenize(example_sent_no_punct)
print("{0:20}{1:20}".format("--word--","--stem--"))
for word in word_tokens:
    print("{0:20}{1:20}".format(word,ps.stem(word)))

example_sent="Python programmers often tend like programming in python
because it's like english. We call people who program in python pythonistas."
example_sent_no_punct=example_sent.translate(str.maketrans("","",string.punctuation))
word_tokens=word_tokenize(example_sent_no_punct)
print("{0:20}{1:20}".format("--word","--lemma"))
for word in word_tokens:
    print("{0:20}{1:20}".format(word,wnl.lemmatize(word,pos="v")))
